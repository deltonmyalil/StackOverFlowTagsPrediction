{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackOverflow Tag Prediction - Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  tags\n",
       "0                How to draw a stacked dotplot in R?                   [r]\n",
       "1  mysql select all records where a datetime fiel...          [php, mysql]\n",
       "2             How to terminate windows phone 8.1 app                  [c#]\n",
       "3  get current time in a specific country via jquery  [javascript, jquery]\n",
       "4                      Configuring Tomcat to Use SSL                [java]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "def read_data(filename, sep='\\t'):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval) # To get rid of the quotes around tags\n",
    "    return data\n",
    "\n",
    "train = read_data('data/train.tsv').head(10000) # Only taking the first 5000 (I have potato PC)\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t') # read_csv because it doesn't have labels, so no literal_eval\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is already partitioned, splitting target and feature\n",
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_validation, y_validation = validation['title'].values, validation['tags'].values\n",
    "X_test =test['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to draw a stacked dotplot in R?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# All the special characters and brackets should be replaced by space\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# tokens that start with numbers, usernames, phone numbers etc\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "try:\n",
    "# Loading stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except Exception:\n",
    "# if stopwords not yet downloaded\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "finally:\n",
    "# English stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "# Now STOPWORDS is a finite sest of string of the stopwords of the english language\n",
    "\n",
    "# Function to prepare the text: Convert to LowerCase, \n",
    "# Replace special chars by space\n",
    "# Replace bad symbols and names starting with numbers,\n",
    "# Replace numbers\n",
    "# Replace stopwords\n",
    "def text_prepare(text): # returns processed text: String\n",
    "    text = text.lower() # converts to lower\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text) # Replacing with space\n",
    "    text = re.sub(BAD_SYMBOLS_RE, '', text) # Replacing with '' ie deleting\n",
    "    \n",
    "    ### The following code does stemming using Porter Stemmer Algo\n",
    "    try:\n",
    "        text = word_tokenize(text)\n",
    "    except Exception:\n",
    "        nltk.download('punkt')\n",
    "        text = word_tokenize(text)\n",
    "    text2 = []\n",
    "    \n",
    "    try:\n",
    "        for literal in text:\n",
    "            text2.append(stemmer.stem(literal)) \n",
    "    except Exception:\n",
    "        nltk.download('wordnet')\n",
    "        for literal in text:\n",
    "            text2.append(stemmer.stem(literal))\n",
    "    text2 = [w for w in text2 if not w in STOPWORDS]\n",
    "    return \" \".join(text2)\n",
    "    \n",
    "    # The following is another way to remove stop words\n",
    "    ''' # The following is one way to remove stopwords\n",
    "    text = text.split(' ') # removing stopwords\n",
    "    new_text = \"\"\n",
    "    for word in text:\n",
    "        if word in STOPWORDS:\n",
    "            text.remove(word)\n",
    "        else:\n",
    "            new_text = new_text + word + \" \"\n",
    "    return new_text[:-1]\n",
    "    ''' \n",
    "    \n",
    "    ### The following code is for lemmatization - replace with stemming\n",
    "    '''\n",
    "    try:\n",
    "        text = word_tokenize(text)\n",
    "    except Exception:\n",
    "        nltk.download('punkt')\n",
    "        text = word_tokenize(text)\n",
    "    text2 = []\n",
    "    \n",
    "    try:\n",
    "        for literal in text:\n",
    "            text2.append(lemmatizer.lemmatize(literal, 'v')) # 'v' for verb based\n",
    "    except Exception:\n",
    "        nltk.download('wordnet')\n",
    "        for literal in text:\n",
    "            text2.append(lemmatizer.lemmatize(literal, 'v'))\n",
    "    text2 = [w for w in text2 if not w in STOPWORDS]\n",
    "    return \" \".join(text2)\n",
    "    '''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the text preparation function to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data processed\n",
      "Validation data processed\n",
      "Test data processed\n"
     ]
    }
   ],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "print(\"Train data processed\")\n",
    "X_validation = [text_prepare(x) for x in X_validation]\n",
    "print(\"Validation data processed\")\n",
    "X_test = [text_prepare(x) for x in X_test]\n",
    "print(\"Test data processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the tags ie target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  10000 in the training set\n",
      "Out of these 44 are distinct\n"
     ]
    }
   ],
   "source": [
    "tags = list(x[0] for x in y_train)\n",
    "print(\"There are \", len(tags), \"in the training set\")\n",
    "print(\"Out of these\", len(set(tags)), \"are distinct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 100000 datapoints in train set and each of them have a tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the ten most common tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c#', 1940),\n",
       " ('java', 1871),\n",
       " ('javascript', 1687),\n",
       " ('php', 1355),\n",
       " ('python', 878),\n",
       " ('c++', 603),\n",
       " ('ruby-on-rails', 302),\n",
       " ('c', 216),\n",
       " ('ios', 164),\n",
       " ('iphone', 156)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tag_occurence_count = Counter(tags)\n",
    "tag_occurence_count.most_common(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the ten most common words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 1157),\n",
       " ('file', 623),\n",
       " ('c', 602),\n",
       " ('java', 578),\n",
       " ('php', 573),\n",
       " ('get', 569),\n",
       " ('valu', 484),\n",
       " ('#', 459),\n",
       " ('error', 440),\n",
       " ('python', 433)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get the word \n",
    "def get_word_list(data):\n",
    "    tokenized_sentences = []\n",
    "    for i in data:\n",
    "        tokenized_sentences.append(word_tokenize(i))\n",
    "    wordList = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            wordList.append(word)\n",
    "    return wordList\n",
    "\n",
    "word_occurence_count = Counter(get_word_list(X_train))\n",
    "word_occurence_count.most_common(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6524 unique words in the training set\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(word_occurence_count), \"unique words in the training set\")  \n",
    "#8200 before lemm # 6969 before stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_df=0.6, min_df=5, ngram_range=(1,2), token_pattern='(\\S+)') #max-1.0, min-10\n",
    "\n",
    "# Transforming data\n",
    "def transform_into_tfidf(data, train_data=False):\n",
    "    if train_data == True: # Fit only for training data\n",
    "        return pd.DataFrame(tfidf.fit_transform(data).todense(), columns=tfidf.get_feature_names())\n",
    "    else:\n",
    "        try:\n",
    "            return pd.DataFrame(tfidf.transform(data).todense(), columns=tfidf.get_feature_names())\n",
    "        except Exception:\n",
    "            print(\"Fit Using train data first to transform test and validation data\")\n",
    "            return -1\n",
    "\n",
    "def tfidf_preprocess_data(X_train, X_validation, X_test):\n",
    "    X_train_tfidf = transform_into_tfidf(data=X_train, train_data=True)\n",
    "    print(\"Fitted and transformed train data\")\n",
    "    X_validation_tfidf = transform_into_tfidf(data=X_validation)\n",
    "    print(\"Transformed validation data\")\n",
    "    X_test_tfidf = transform_into_tfidf(data=X_test)\n",
    "    print(\"Transformed test data\")\n",
    "    return X_train_tfidf, X_validation_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed train data\n",
      "Transformed validation data\n",
      "Transformed test data\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_validation_tfidf, X_test_tfidf = tfidf_preprocess_data(X_train, X_validation, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th># aspnet</th>\n",
       "      <th># code</th>\n",
       "      <th># get</th>\n",
       "      <th># net</th>\n",
       "      <th># use</th>\n",
       "      <th># window</th>\n",
       "      <th>+</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yii</th>\n",
       "      <th>yii2</th>\n",
       "      <th>youtub</th>\n",
       "      <th>zend</th>\n",
       "      <th>zend framework</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip file</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 2207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     #  # aspnet  # code  # get  # net  # use  # window    +    0    1  ...   \\\n",
       "0  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "1  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "2  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "3  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "4  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "5  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "6  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "7  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "8  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "9  0.0       0.0     0.0    0.0    0.0    0.0       0.0  0.0  0.0  0.0  ...    \n",
       "\n",
       "   yet  yii  yii2  youtub  zend  zend framework  zero  zip  zip file  zoom  \n",
       "0  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "1  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "2  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "3  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "4  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "5  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "6  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "7  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "8  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "9  0.0  0.0   0.0     0.0   0.0             0.0   0.0  0.0       0.0   0.0  \n",
       "\n",
       "[10 rows x 2207 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.head(10) # 2299 before lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the label - OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/themadscientist/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:935: UserWarning: unknown class(es) ['ajax', 'algorithm', 'angularjs', 'apache', 'class', 'cocoa-touch', 'codeigniter', 'csv', 'date', 'datetime', 'dom', 'entity-framework', 'facebook', 'file', 'forms', 'google-maps', 'hibernate', 'html5', 'image', 'jsp', 'laravel', 'linq', 'loops', 'maven', 'mongodb', 'multithreading', 'node.js', 'numpy', 'oop', 'opencv', 'osx', 'pandas', 'parsing', 'performance', 'pointers', 'python-2.7', 'python-3.x', 'qt', 'rest', 'ruby-on-rails-3', 'selenium', 'servlets', 'session', 'sockets', 'sorting', 'spring', 'spring-mvc', 'swing', 'twitter-bootstrap', 'validation', 'visual-studio', 'visual-studio-2010', 'wcf', 'web-services', 'winforms', 'xaml'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=sorted(tag_occurence_count.keys()))\n",
    "\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_validation = mlb.transform(y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = OneVsRestClassifier(LogisticRegression(solver='lbfgs')).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_validation = model.predict(X_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.30823333333333336\n",
      "F1:  0.5408044366436083\n",
      "ROC_AUC  0.6902026735691748\n",
      "Average Precision  0.22893354893263507\n",
      "Recall aka sensitivity  0.3815162424822592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_validation, y_pred_validation))\n",
    "print(\"F1: \", f1_score(y_validation, y_pred_validation, average='micro'))\n",
    "print(\"ROC_AUC \", roc_auc_score(y_validation, y_pred_validation, average='micro'))\n",
    "print(\"Average Precision \", average_precision_score(y_validation, y_pred_validation))\n",
    "print(\"Recall aka sensitivity \", recall_score(y_validation, y_pred_validation, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "\n",
    "## This snippet to plot wont work with multilabel\n",
    "'''\n",
    "fpr, tpr, thresholds = roc_curve(y_pred_validation, y_validation)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This being a 44 labelled classification, baseline is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022727272727272728"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, an accuracy of 30.32% is not bad. ROC_AUC of 70% is also pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research team who published the paper got an F1 of .60 by taking 100% of title data only - Good for them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make automated scrips to take data, preprocess and predict the outcome\n",
    "2. Apply various other techniques - word embeddings, DeepNLP, and other traditional ML Algorithms as well (think SVM)\n",
    "3. Buy a better PC that can load 100% of the data as opposed to the 10% I loaded now to train set\n",
    "4. Try this using the college GPUs with 100% of the dataset - both title and post texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project follows the research paper: Autonomous Tagging of Stack Overflow Questions by Mihail Eric, Ana Klimovic, Victor Zhong\n",
    "http://cs229.stanford.edu/proj2014/Mihail%20Eric,%20Ana%20Klimovic,%20Victor%20Zhong,MLNLP-Autonomous%20Tagging%20Of%20Stack%20Overflow%20Posts.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to self:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn how to plot multiclass ROCs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
